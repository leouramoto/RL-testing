{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreating \"Playing Atari with Deep Reinforcement Learning\" paper by DeepMind Technologies\n",
    "\n",
    "In short this notebook attempts to recreate the Deep Q Network paper by DeepMind. Because DeepMind has computational capabilities that surpass anything a regular individual might have access to some changes are made. The main change to is with an attempt to penalize dying in the hopes of the agent learning to avoid enemy fire. This does not turn out to be beneficial with the amount of testing that was done with the computational power available to me. I ran this notebook iterating over 1000 games while the original paper ran the algorithm for 10 million games. Perhaps with numbers nearing the latter the dying penalty might have had an impact on the behavior.\n",
    "\n",
    "## Libraries you need\n",
    "\n",
    "#### OpenAI Gym\n",
    "You must have both OpenAI Gym and Gym[atari] installed. \n",
    "\n",
    "To install: \"pip install gym\" and \"pip install gym[atari]\"\n",
    "\n",
    "### Keras, Tensorflow\n",
    "Running this is very slow so GPU versions are recommended. However getting the GPU accelerated packages to work is definitely not a trivial thing so feel free to use your CPU! The notebook lets you know if you are running it in GPU mode so it's also a check on if your installation has been done correctly. There is a FORCE_CPU parameter that makes tensorflow use the CPU.\n",
    "\n",
    "To install: \"pip install keras\" and \"pip install tensorflow\"\n",
    "\n",
    "### Matplotlib\n",
    "Plotting how the agent learns is fun! \n",
    "\n",
    "To install: \"pip install matplotlib\"\n",
    "\n",
    "### numba\n",
    "Use numba to reset the GPU, this can be skipped if not ran on the GPU\n",
    "\n",
    "To install: \"pip install numba\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import envs\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import cuda\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if tensorflow is indeed running on the GPU and how many GPUs we have available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_devices = device_lib.list_local_devices()\n",
    "print(my_devices)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU issues\n",
    "\n",
    "Sometimes running things on the GPU is a pain and causes issues such as running out of memory, you can just force the notebook to use CPU if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_CPU = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the paper suggests this model can be tested with a bunch of games in the Arcade Learning Environment (ale). Choose here which one to run. More games can easily be added to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALE_GAMES = [\"SpaceInvaders-v0\", \"Phoenix-v0\",\"Venture-v0\"]\n",
    "GAME_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to reset the GPU as it does have a tendency to get locked in Jupyter\n",
    "def gpu_reset():\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "# If FORCE_GPU is set on then remove GPUs from devices tensorflow can see\n",
    "if(FORCE_CPU):\n",
    "    tf.config.experimental.set_visible_devices(devices= [], device_type='GPU')\n",
    "else:\n",
    "    gpu_reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the environment and record the observation space sizes and the number of possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME_NAME = ALE_GAMES[GAME_ID]\n",
    "MODEL_NAME = GAME_NAME + \"_model\"\n",
    "CONST_LIVES_KEY = 'ale.lives'\n",
    "env = gym.make(GAME_NAME)\n",
    "state = env.reset()\n",
    "obspace = env.observation_space\n",
    "HEIGHT = obspace.shape[0]\n",
    "WIDTH = obspace.shape[1]\n",
    "ACTION_COUNT = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function - Take an RGB frame and make a BW representation of it. The frame has its data in columns which is then split into pixels each consisting of R,G and B values. The frame, initially HEIGHT x WIDTH, can be cropped to a more suitable size. The original paper first downsamples the frame to 110 x 84 size and then crops it to 84 x 84.\n",
    "\n",
    "Here we will skip the downsampling and instead crop the image from the center to a rectangle, a square of WIDTH x WIDTH size is a reasonable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_HEIGHT = WIDTH\n",
    "TARGET_WIDTH = WIDTH\n",
    "\n",
    "\n",
    "TOPCROP = int((HEIGHT - TARGET_HEIGHT)/2)\n",
    "LEFTCROP = int((WIDTH - TARGET_WIDTH)/2)\n",
    "\n",
    "def bw_normalize(frame):\n",
    "    frame = frame/255\n",
    "    frame = np.mean(frame, 2, keepdims=True)\n",
    "    return frame[TOPCROP:TOPCROP+TARGET_HEIGHT, LEFTCROP:LEFTCROP+TARGET_WIDTH]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the desired behavior for saving and loading models here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD = False\n",
    "SAVE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning related hyperparameters. \n",
    "Learning happens with the following logic:\n",
    "A replay buffer is created, containing data for the current frame, action taken, reward received, whether or not the frame was terminal and the next frame. \n",
    "\n",
    "After each step the data is recorded and added to the replay buffer with a size of MEMORY_SIZE. The memory buffer will keep its size below MEMORY_SIZE by throwing out the oldest data.\n",
    "\n",
    "Next a learning step is done. A random sample of SAMPLES is sampled from the replay buffer. The Q network is fit to predict Q(s,a) + r + b max Q'(s', . ), s is the state, a the taken action, b the discount factor, s' the next state, and Q' a network that is periodically updated to match Q. \n",
    "\n",
    "Exploration is done with epsilon-greedy strategy with linear epsilon-decay. The initial value of epsilon, rate at which random actions are taken, is initially set to EPSILON_INIT and it then linearly decays with each game towards EPSILON_END reaching it at the final iteration. In the original paper the approach was to train the model with linear epsilon decay for one million games and then for an additional 9 million games with epsilon = 0.1. This is beyond the computational capabilities of normal people and as such we will perform only the epsilon decay portion of the training and with a much smaller number of games.\n",
    "\n",
    "It is possible to set a penalty for dying, with the hope that this teaches the agent to avoid enemy shots. This requires the last activation layer to be linear as an all-relu network would fail to predict negative values.\n",
    "\n",
    "Every PERFORMANCE_RECORD_RATE iterations the performance of the model is measured by having it play the game once. As displaiyng the playing slows the performanc down a little, the DRAW_RATE parameter controls how often the performance measuring games are drawn with only one game in DRAW_RATE is drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_COPY_RATE = 4                                # How often to copy weights from Q' to Q\n",
    "SAMPLES = 256                                       # How many samples to take from the memory\n",
    "BATCH_SIZE = 32                                     # Batch size for model.train()\n",
    "ATTEMPTS = 1000                                     # Number of times to play the game\n",
    "MEMORY_SIZE = 20000                                 # Size of the memory buffer\n",
    "SKIPFRAMES = 2                                      # How many frames are skipped between actions / snapshots\n",
    "GAMMA = 0.99                                        # Learning reward decay\n",
    "EPSILON_INIT = 1                                    # Initial exploration rate\n",
    "EPSILON_END = 0.1                                   # Final exploration rate\n",
    "DIE_PENALTY = 100                                   # Penalize dying. Requires changing the network as ReLUs can't predict negative values!\n",
    "PERFORMANCE_RECORD_RATE = 5                         # How often to record performance\n",
    "PERFORMANCE_RECORD_SAMPLES = 5                      # How many samples to take when performance is recorded\n",
    "DRAW_RATE = 10000                                   # How often do draw performance recording, must be a multiple of PERFORMANCE_RECORD_RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "The model is a convolutional neural network with 2convolutional layers followed by a dense layer. The output has ACTION_COUNT neurons and the predicted value is the Q-value associated with each action.\n",
    "\n",
    "The original paper used 16 8x8 filters with stride 4 followed by a layer of 32 4x4 filters with stride 2.\n",
    "\n",
    "The input consists of FRAME_COUNT last frames, in the paper this was 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_COUNT = 4                                      # Number of frames the model will consider, make > 1 to consider past screens too\n",
    "BORDER_MODE = 'valid'                                # Border mode for convolutions\n",
    "CONVSIZE_1 = 8                                       # Convolution 1 size\n",
    "CONVSIZE_2 = 4                                       # Convolution 2 size\n",
    "STRIDE_1 = 4                                         # Convolution 1 stride\n",
    "STRIDE_2 = 2                                         # Convolution 2 stride\n",
    "CONV_FILTERS_1 = 16                                  # Convolution 1 filter count\n",
    "CONV_FILTERS_2 = 32                                  # Convolution 2 filter count\n",
    "FINAL_NEURONS = 256                                  # Dense layer neuron count\n",
    "LEARNING_RATE = 0.001                                # Learning rate, might need hand-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(CONV_FILTERS_1, CONVSIZE_1, STRIDE_1, padding = BORDER_MODE, input_shape=(TARGET_HEIGHT, TARGET_WIDTH, FRAME_COUNT)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Conv2D(CONV_FILTERS_2, CONVSIZE_2, STRIDE_2, padding = BORDER_MODE))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(FINAL_NEURONS))\n",
    "    # The final activation must be linear if dying penalty is used\n",
    "    # Otherwise the model breaks as it attempts to predict negative numbers in an all-ReLU network\n",
    "    # If no dying penalty is used relu is prefered.\n",
    "    if(DIE_PENALTY>0):\n",
    "        model.add(keras.layers.Activation('linear'))\n",
    "    else:\n",
    "        model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dense(ACTION_COUNT))\n",
    "    model.compile(loss='mse',optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the behavior chosen above the model is either loaded or a new one is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(LOAD):\n",
    "    model = load_model(MODEL_NAME)\n",
    "else:\n",
    "    model = construct_network()\n",
    "model_target = construct_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buffer containing MEMORY_SIZE last frames, actions and rewards represented as its own class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, buffersize):\n",
    "        self.buffersize = buffersize\n",
    "        self.count = 0\n",
    "        self.buffer = collections.deque()\n",
    "\n",
    "    def add(self, state, action, reward, terminal, nextstate):\n",
    "        exp = (state, action, reward,terminal, nextstate)\n",
    "        if(self.count < self.buffersize):\n",
    "            self.buffer.append(exp)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(exp)\n",
    "            \n",
    "    def sample(self, size):\n",
    "        res = []\n",
    "        if(self.count < size):\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, size)\n",
    "        state_batch, action_batch, reward_batch, terminal_batch, nextstate_batch = map(np.array, zip(*batch))\n",
    "        return state_batch, action_batch, reward_batch, terminal_batch, nextstate_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the play and recording performance\n",
    "\n",
    "Define functions for running the game with a model or with purely random actions to visualize what the agent has learned so far. The render parameter controls whether or not the game is rendered to a window of if it's run as a background process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(model, render):\n",
    "    sys.stdout.write('\\r ---------------------------------------------')\n",
    "    states = collections.deque()\n",
    "    state = bw_normalize(env.reset())\n",
    "    for k in range(FRAME_COUNT):\n",
    "        states.append(state)\n",
    "    action = 0\n",
    "    score=0\n",
    "    newstate = env.step(action)\n",
    "    skipframe = 0\n",
    "    while(newstate[2]==False):\n",
    "        skipframe +=1\n",
    "        if(skipframe < SKIPFRAMES):\n",
    "            newstate = env.step(action)\n",
    "            score += newstate[1]\n",
    "            continue\n",
    "        data = np.array(states).reshape(-1,TARGET_HEIGHT,TARGET_WIDTH,FRAME_COUNT)\n",
    "        actions = model.predict(data)\n",
    "        action = np.argmax(actions)\n",
    "        sys.stdout.write('\\r'+\"Game action: {0}\".format(action))\n",
    "        newstate = env.step(action)\n",
    "        nextstate = bw_normalize(newstate[0])\n",
    "        score += newstate[1]\n",
    "        states.popleft()\n",
    "        states.append(nextstate)\n",
    "\n",
    "        if(render):\n",
    "            env.render()\n",
    "        skipframe = 0\n",
    "    sys.stdout.write('\\r ---------------------------------------------')\n",
    "    return score\n",
    "\n",
    "def run_game_random(render):\n",
    "    action = 0\n",
    "    score=0\n",
    "    newstate = env.step(action)\n",
    "    skipframe = 0\n",
    "    while(newstate[2]==False):\n",
    "        skipframe +=1\n",
    "        if(skipframe < SKIPFRAMES):\n",
    "            newstate = env.step(action)\n",
    "            score += newstate[1]\n",
    "        action = random.randint(0,ACTION_COUNT-1)\n",
    "        sys.stdout.write('\\r'+\"Game action: {0}\".format(action))\n",
    "        newstate = env.step(action)\n",
    "        nextstate = bw_normalize(newstate[0])\n",
    "        score += newstate[1]\n",
    "        states.popleft()\n",
    "        states.append(nextstate)\n",
    "\n",
    "        if(render):\n",
    "            env.render()\n",
    "        skipframe = 0\n",
    "    sys.stdout.write('\\r ---------------------------------------------')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning\n",
    "\n",
    "Now it's time for the actual learning algorithm. Initialize an array for history so we can record how our agent learned and initialze the ReplayMemory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "replaybuffer = ReplayMemory(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(1,ATTEMPTS):\n",
    "    # Initialize new game, update epsilon and set up buffers\n",
    "    epsilon =(EPSILON_INIT-EPSILON_END) * (ATTEMPTS-iteration)/ATTEMPTS + EPSILON_END\n",
    "    sys.stdout.write('\\r ---------------------------------------------')\n",
    "    states = collections.deque()\n",
    "    state = bw_normalize(env.reset())\n",
    "    for k in range(FRAME_COUNT):\n",
    "        states.append(state)\n",
    "    action = 0\n",
    "    newstate = env.step(action)\n",
    "    states.popleft()\n",
    "    states.append(bw_normalize(newstate[0]))\n",
    "    frame_number = 1\n",
    "    skipframe = 0\n",
    "    reward = 0\n",
    "    game_ended = False\n",
    "    lifecounter = newstate[3][CONST_LIVES_KEY]\n",
    "    old_data = np.array(states).reshape(-1,TARGET_HEIGHT,TARGET_WIDTH,FRAME_COUNT)\n",
    "    new_data = np.array(states).reshape(-1,TARGET_HEIGHT,TARGET_WIDTH,FRAME_COUNT)\n",
    "    # Everything initialized, run game loop\n",
    "    while(game_ended == False):\n",
    "        # Choose and perform the next action\n",
    "        frame_number += 1\n",
    "        skipframe +=1\n",
    "        # Make decisions only every SKIPFRAMES frame\n",
    "        if(skipframe > SKIPFRAMES):\n",
    "            actions = model.predict(new_data)\n",
    "            action = np.argmax(actions)\n",
    "            if(random.uniform(0,1)<epsilon):\n",
    "                action = random.randint(0,ACTION_COUNT-1)\n",
    "            skipframe = 0\n",
    "        sys.stdout.write('\\r'+\"Game iteration: {0}, frame {1}, action {2}\".format(iteration,frame_number, action))\n",
    "        newstate = env.step(action)\n",
    "        reward += newstate[1]\n",
    "        if(lifecounter > newstate[3][CONST_LIVES_KEY]):\n",
    "            reward -= DIE_PENALTY\n",
    "        lifecounter = newstate[3][CONST_LIVES_KEY]\n",
    "        \n",
    "        # Update the states      \n",
    "        nextscreen = bw_normalize(newstate[0])\n",
    "        states.popleft()\n",
    "        states.append(nextscreen)\n",
    "                \n",
    "        # Store the transition\n",
    "        old_data = new_data\n",
    "        new_data = np.array(states).reshape(-1,TARGET_HEIGHT,TARGET_WIDTH,FRAME_COUNT)\n",
    "        game_ended = newstate[2]\n",
    "        replaybuffer.add(old_data, action, reward, not game_ended, new_data)\n",
    "        \n",
    "    sys.stdout.write('\\r ---------------------------------------------')\n",
    "    sys.stdout.write('\\r Training')\n",
    "    # Random mini batch from replaybuffer and fit\n",
    "    state_batch, action_batch, reward_batch, terminal_batch, newdata_batch = replaybuffer.sample(SAMPLES)\n",
    "    batchsize_actual = len(newdata_batch)\n",
    "    state_batch = state_batch.reshape(batchsize_actual, TARGET_HEIGHT, TARGET_WIDTH, FRAME_COUNT)\n",
    "    newdata_batch = newdata_batch.reshape(batchsize_actual, TARGET_HEIGHT, TARGET_WIDTH, FRAME_COUNT)\n",
    "    # Update only new information\n",
    "    targets = model_target.predict(state_batch)\n",
    "    # Use old estimation for future value\n",
    "    predictions = model.predict(newdata_batch)\n",
    "    # Make sure the model is not broken and predicting NaN values\n",
    "    a = np.sum(targets.reshape(-1))\n",
    "    b = np.sum(predictions.reshape(-1))\n",
    "    if(np.isnan(a) or np.isnan(b)):\n",
    "        sys.stdout.write('\\r ERROR: Broken model - too high learning rate?')\n",
    "        break\n",
    "    for i in range(len(action_batch)):\n",
    "        targets[i,action_batch] = reward_batch[i] + int(terminal_batch[i]) * GAMMA * np.max(predictions[i])\n",
    "    model_target.fit(state_batch.reshape(batchsize_actual, TARGET_HEIGHT, TARGET_WIDTH, FRAME_COUNT), targets, batch_size=BATCH_SIZE, verbose=0)\n",
    "    if(iteration % WEIGHT_COPY_RATE == 0):\n",
    "        model.set_weights(model_target.get_weights())\n",
    "    sys.stdout.write('\\r Measuring performance')\n",
    "    if(iteration % PERFORMANCE_RECORD_RATE==0):\n",
    "        render = False\n",
    "        if(iteration % DRAW_RATE == 0):\n",
    "            render = True\n",
    "        evaluated = []\n",
    "        for measurement in range(PERFORMANCE_RECORD_SAMPLES):\n",
    "            evaluated += [run_game(model, render)]\n",
    "        history += [[iteration, np.mean(evaluated)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(SAVE):\n",
    "    model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for h in history:\n",
    "    x += h[0]\n",
    "    y += h[1]\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the resulting model\n",
    "\n",
    "We can now compare the trained model to a random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "run_game(model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "run_game_random(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "This notebook added the concept penalizing dying in the attempt to have the agent learn to dodge. However this did not quite work as expected and could certainly be improved upon. More iterations would most likely help, but perhaps there is a better strategy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
